{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 505 Homework 05:  Recurrent Neural Networks\n",
    "\n",
    "#### Due Monday  11/27 at midnight (1 minute after 11:59 pm) in Gradescope (with a grace period of 6 hours)\n",
    "#### You may submit the homework up to 24 hours late (with the same grace period) for a penalty of 10%. \n",
    "\n",
    "All homeworks will be scored with a maximum of 100 points; point values are given\n",
    "for individual problems, and if parts of problems do not have point values given, they\n",
    "will be counted equally toward the total for that problem. \n",
    "\n",
    "Note: This homework is a bit different from the first four in this class in that in some parts we are specified **what** you need to do for your solutions, but much less of the **how** you write the details of the code. There are three reasons for this:\n",
    "\n",
    "- In a graduate level CS class, after four homeworks and two months of lectures, you should be well-equipped to work out the coding issues for yourself, and in general, going forward, this is how you will solve the kinds of problems presented here; \n",
    "- Suggestions for resources (mostly ML blogs) will be suggested; there are many resources, but these are from bloggers that I trust and have used in the past;\n",
    "- I am expecting that you will make good use of chatGPT for help with the details of syntax and low-level organization of your code. There is often nothing very stimulating or informative about precisely what is the syntax needed for a particular kind of layer in a network, and rather than poke around on StackOverflow, chatGPT is particularly good at summarizing existing approaches to ML coding tasks. \n",
    "\n",
    "#### Submission Instructions\n",
    "\n",
    "You must complete the homework by editing <b>this notebook</b> and submitting the following two files in Gradescope by the due date and time:\n",
    "\n",
    "  - A file <code>HW05.ipynb</code> (be sure to select <code>Kernel -> Restart and Run All</code> before you submit, to make sure everything works); and\n",
    "  - A file <code>HW05.pdf</code> created from the previous.\n",
    "  \n",
    "  For best results obtaining a clean PDF file on the Mac, select <code>File -> Print Review</code> from the Jupyter window, then choose <code>File-> Print</code> in your browser and then <code>Save as PDF</code>.  Something  similar should be possible on a Windows machine -- just make sure it is readable and no cell contents have been cut off. Make it easy to grade!\n",
    "  \n",
    "The date and time of your submission is the last file you submitted, so if your IPYNB file is submitted on time, but your PDF is late, then your submission is late. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborators (5 pts)\n",
    "\n",
    "Describe briefly but precisely\n",
    "\n",
    "1. Any persons you discussed this homework with and the nature of the discussion;\n",
    "2. Any online resources you consulted and what information you got from those resources; and\n",
    "3. Any AI agents (such as chatGPT or CoPilot) or other applications you used to complete the homework, and the nature of the help you received. \n",
    "\n",
    "A few brief sentences is all that I am looking for here. \n",
    "\n",
    "    chatGPT: help me with debugging of each problem; summarize approaches and guidelines for each problem; syntax of pytorch\n",
    "    StackOverflow: also help me with debugging and remind me of the complete chunk of code that others wrote on the same topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yfsong/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import shuffle, seed, choice\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split,Dataset,DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem One:  Character-Level Generative Model (20 pts)\n",
    "\n",
    "A basic character-level model has been provided on the class web site in the row for Lecture 14: \n",
    "<a href=\"https://www.cs.bu.edu/fac/snyder/cs505/CharacterLevelLSTM.ipynb\">IPYNB</a>. Your first step is to download this and run it in Colab (or download the data file, which is in the CS 505 Data Directory and also linked on the web site, and run it on your local machine) and understand all its various features. Most of it is straight-forward at this point in the course, but the definition of the model is a bit messy, and you will need to read about LSTM layers in the Pytorch documents to really understand what it is doing and what the hyperparameters mean. \n",
    "\n",
    "Also take a look at the article \"The Unreasonable Effectiveness of Recurrent Neural Networks\" linked with lecture 14. \n",
    "\n",
    "For this problem, you will run this code on a dataset consisting of Java code files, which has been uploaded to the CS 505 Data Directory and also to the class web site: <a href=\"https://www.cs.bu.edu/fac/snyder/cs505/JavaFiles/\">DIR</a>  Select some number of these files and concatenate them into one long text file, such that you have approximately 10-20K characters (if you have trouble running out of RAM you can use fewer, but try to get at least 10K). \n",
    "\n",
    "You will run the character-level model on this dataset. You may either cut and paste code into this notebook, or submit the file with your changes and output along with this notebook to Gradescope.\n",
    "\n",
    "Your task is to get a character-level model that has not simply memorized the Java text file by overfitting, and does not do much other than spit out random characters (underfitting).  You will get the former if you simply run it for many epochs without any changes to the hyperparameters; you will get the latter if you run it only a few epochs. \n",
    "\n",
    "You should experiment with different hyperparameters, which in the notebook are indicated\n",
    "by \n",
    "\n",
    "          <== something to play with\n",
    "\n",
    "and try to get a model that seems to recognize typical Java syntax such as comments, matching parentheses, expressions, assignments, and formatting, but is not just repeating\n",
    "exact text from the data file. Clearly, the number of epochs plays a crucial role, but I also want you to\n",
    "experiment with the various hyperparameters to try to avoid overfitting. See my lectures on T 10/31 and Th 11/2 (recorded and on my YT channel) for the background to this.\n",
    "\n",
    "Note that the code you will work from does not use validation and testing sets, nor does it calculate the accuracy, but only tracks the loss. The nature of the data sets for character-level models does not seem to lend itself to accuracy metrics, but you may wish to try this -- I have not found it to be useful, but have simply focussed on the output and \"eyeballed\" the results to determine how much they have generalized\n",
    "from the data. \n",
    "\n",
    "Submit your notebook(s) to Gradescope as usual, and also provide a summary of your results in the next cell. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "best parameters:\n",
    "- hidden: 128\n",
    "- n_layers: 1\n",
    "- dropout: 0.0\n",
    "- learning_rate (lr): 0.001\n",
    "- epochs: 30\n",
    "- batch_size: 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnB5z-ZoZEa5"
   },
   "source": [
    "### Your analysis\n",
    "\n",
    "Please describe your experiments and cut and paste various outputs to show how the model performed at\n",
    "various numbers of epochs and with various hyperparameters. What characteristics of Java was it able to learn? What did it not learn? The article \"The Unreasonable ...\" does a nice job of showing this kind of behavior as the number of epochs increases, and you might look at it before writing your answer here. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "- Hyperparameters: The model was trained with varying configurations of hidden dimensions, LSTM layers, dropout rates, learning rates, epochs, and batch sizes.\n",
    "\n",
    "**Best Performing Parameters**\n",
    "- Hidden Dimension: 128\n",
    "- Number of LSTM Layers: 1\n",
    "- Dropout Rate: 0.0\n",
    "- Learning Rate: 0.001\n",
    "- Number of Epochs: 30\n",
    "- Batch Size: 64\n",
    "\n",
    "**Analysis of Model Performance**\n",
    "- Epochs 1-10: Initially, the model produced mostly random characters with occasional recognizable Java syntax elements like semicolons, curly braces, and basic keywords (public, class, void). This indicated the beginning of pattern learning.\n",
    "- Epochs 10-20: The model started forming more coherent structures. It began to respect the Java syntax more consistently, correctly placing parentheses and braces. However, logical coherence in the code was still lacking.\n",
    "- Epochs 20-30: The model showed significant improvement. It was able to generate syntactically correct statements, including control structures (if, for) and method declarations. The generated code began to resemble functional Java snippets, though with some logical inconsistencies and occasional syntax errors.\n",
    "\n",
    "**Learning Characteristics**\n",
    "- The model learned basic Java syntax, such as correct placement of braces, semicolons, and method structures.\n",
    "- Common Java keywords were appropriately used, indicating a grasp of Java's lexical elements.\n",
    "- The generation of loops and conditional statements showed a more profound understanding of code flow in Java.\n",
    "\n",
    "**Limitations**\n",
    "- While the syntax was often correct, the model struggled with logical coherence. The generated code blocks did not always make logical sense.\n",
    "- Advanced Java concepts like generics, lambdas, or complex class structures were not well-represented in the generated code\n",
    "- Comment Generation: The model had difficulty generating meaningful comments, often producing fragmented or irrelevant comments.\n",
    "\n",
    "<br>\n",
    "Similar to findings in \"The Unreasonable Effectiveness of Recurrent Neural Networks\", the model showed a progression from learning basic syntax to more complex structures as the number of epochs increased. The model's limitations in generating logically coherent and complex code structures align with the observations made in the article about the challenges faced by character-level models in capturing higher-level abstractions.\n",
    "<br>\n",
    "The hyperparameter tuning successfully led to a model that could generate Java-like code, demonstrating a good understanding of Java's syntax and basic structures. However, it fell short in generating logically coherent and complex code, highlighting the inherent limitations of character-level generative models in understanding higher-level programming constructs and logic. Future work might involve integrating more advanced techniques or shifting towards token-level models to capture more complex and coherent code structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Two:  Word-Level Generative Model (40 pts)\n",
    "\n",
    "In this problem you will write another generative model, as you did in HW 03, but this time you will use an LSTM network, GloVe word embeddings, and beam search. \n",
    "\n",
    "Before you start, read the following blog post to see the core ideas involved in creating a generative model using word embeddings:\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
    "\n",
    "You may also wish to consult with chatGPT about how to develop this kind of model in Pytorch.\n",
    "\n",
    "The requirements for this problem are as follows (they mostly consist of the extensions proposed at\n",
    "the end of the blog post linked above):\n",
    "\n",
    "- Develop your code in Pytorch, not Keras\n",
    "- Use the novel *Persuation* by Jane Austen as your training data (available through the NLTK, you can just grab the sentences using `nltk.corpus.gutenberg.sents('austen-persuasion.txt')`); if you have trouble with RAM you will need to cut down the number of sentences (perhaps by eliminating the longest sentences as well, see next point). \n",
    "- Develop a sentence-level model by padding sentences to the maximum sentence length in the novel (if this seems extreme, you may wish to delete a small number of the longest sentences to reduce the maximum length). Surround your data sentences with `<s>` and `</s>` and your model should generate one sentence at a time (as you did in HW 03), i.e., it should stop if it generates the `</s>` token. \n",
    "- Use pretrained GLoVe embeddings with dimension 200, and update them (refine by training further) on the sentences in the novel; if you have trouble with RAM you may use a smaller dimension. \n",
    "- Experiment with the hyperparameters (sample length, number of layers, uni- or bi-directional, weight_decay, dropout, number of epochs, temperature of the softmax, etc.) as you did in Problem One to find the \"sweet spot\" where you are generating interesting-looking sentences but not simply repeating sentences from the data. You may want to try adding more linear layers on top to pick the most likely next word. \n",
    "- Generate sentences using Beam Search, which we describe below. \n",
    "\n",
    "Your solution should be the code, samples of sentences generated with their score (described below), and your description of the investigation of various hyperparameters, and what strategy ended up seeming to generate the most realistic sentences that were not simply a repeat of sentences in the data. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnB5z-ZoZEa5"
   },
   "source": [
    "### Beam Search\n",
    "\n",
    "Beam search was described, and example shown, in Lecture 14. Here is a brief pseudo-code explaination of what\n",
    "you need to do:\n",
    "\n",
    "1. Develop your code as described above so that it can generate single sentences;\n",
    "2. Copy enough of your code over from HW 03 so that you can calculate the perplexity of\n",
    "        sentences (using the entire novel, or perhaps even a number of Jane Austen's novels as\n",
    "        the data source). As an alternative, you may wish to do this separately, store the nested dictionary\n",
    "        using Pickle, and load it here. \n",
    "3. Calculate the probability distribution of sentences in your data source that you used in the previous step, similar to what you did at the end of HW 01. \n",
    "4. Create a \"goodness function\" which estimates the quality of a sentence as the perplexity times the probability of its length.  This will be applied to all sequences of words, and not just sentences, but as a first approximation this is a way to attempt to make the distribution of sentence lengths similar to that in the novel.\n",
    "5. Follow the description in slide 7 of Lecture 14 to generate until you have 10 finished sentences. Print these out with their perplexity, probability of their length, and the combined goodness metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/yfsong/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "sentences = gutenberg.sents('austen-persuasion.txt')\n",
    "\n",
    "# preprocess sentences\n",
    "# add <s> and </s> tokens to each sentence\n",
    "processed_sentences = [['<s>'] + s + ['</s>'] for s in sentences]\n",
    "\n",
    "# flatten the list for tokenizer\n",
    "all_words = [word for sentence in processed_sentences for word in sentence]\n",
    "\n",
    "# tokenize words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_words)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# convert sentences to sequences\n",
    "sequences = tokenizer.texts_to_sequences(processed_sentences)\n",
    "\n",
    "# pad sequences to the same length\n",
    "max_len = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "with open('glove.6B.200d.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = True  # Allow embeddings to be updated\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "# create the model\n",
    "model = LSTMModel(vocab_size, 200, hidden_dim=256, embedding_matrix=embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/syshz9v92js62ld8ch0rw37r0000gn/T/ipykernel_51535/879937104.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequences = torch.tensor(sequences, dtype=torch.long)\n",
      "/var/folders/vr/syshz9v92js62ld8ch0rw37r0000gn/T/ipykernel_51535/879937104.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_seq = torch.tensor(sequence[:-1]).unsqueeze(0)  # exclude the last token for input\n",
      "/var/folders/vr/syshz9v92js62ld8ch0rw37r0000gn/T/ipykernel_51535/879937104.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(sequence[1:]).unsqueeze(0)  # exclude the first token for target\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2400.5825868116553\n",
      "Epoch 1, Loss: 2084.580128505808\n",
      "Epoch 2, Loss: 1856.0351787484997\n",
      "Epoch 3, Loss: 1647.644096794308\n",
      "Epoch 4, Loss: 1470.4520426218746\n",
      "Epoch 5, Loss: 1326.04761444772\n",
      "Epoch 6, Loss: 1209.7980210971982\n",
      "Epoch 7, Loss: 1107.4174386351424\n",
      "Epoch 8, Loss: 1027.6993681492563\n",
      "Epoch 9, Loss: 959.421238313859\n",
      "Epoch 10, Loss: 903.5411925513075\n",
      "Epoch 11, Loss: 855.8651492945769\n",
      "Epoch 12, Loss: 828.7267522198433\n",
      "Epoch 13, Loss: 800.7799267702774\n",
      "Epoch 14, Loss: 775.1614035736097\n",
      "Epoch 15, Loss: 751.8618530005986\n",
      "Epoch 16, Loss: 734.0645840982176\n",
      "Epoch 17, Loss: 710.6331725140934\n",
      "Epoch 18, Loss: 693.0512129584705\n",
      "Epoch 19, Loss: 692.3999641963896\n",
      "Epoch 20, Loss: 687.4057320321715\n",
      "Epoch 21, Loss: 662.610612432437\n",
      "Epoch 22, Loss: 637.8380385693281\n",
      "Epoch 23, Loss: 656.8921187023525\n",
      "Epoch 24, Loss: 761.7054781995344\n",
      "Epoch 25, Loss: 706.6052053667925\n",
      "Epoch 26, Loss: 689.7453429465307\n",
      "Epoch 27, Loss: 639.4171176555732\n",
      "Epoch 28, Loss: 602.1793355508859\n",
      "Epoch 29, Loss: 590.0827057752743\n",
      "Epoch 30, Loss: 573.1652416516939\n",
      "Epoch 31, Loss: 599.1546789754658\n",
      "Epoch 32, Loss: 605.3887136559075\n",
      "Epoch 33, Loss: 622.5952373807467\n",
      "Epoch 34, Loss: 607.8140804719457\n",
      "Epoch 35, Loss: 611.0174987145313\n",
      "Epoch 36, Loss: 586.9983934997454\n",
      "Epoch 37, Loss: 583.2975747211796\n",
      "Epoch 38, Loss: 585.0604012797659\n",
      "Epoch 39, Loss: 583.8742922176274\n",
      "Epoch 40, Loss: 602.1308299412078\n",
      "Epoch 41, Loss: 546.6950455696277\n",
      "Epoch 42, Loss: 544.4182272540476\n",
      "Epoch 43, Loss: 529.7941123470921\n",
      "Epoch 44, Loss: 554.5145645216388\n",
      "Epoch 45, Loss: 522.0148114572892\n",
      "Epoch 46, Loss: 534.8334245445285\n",
      "Epoch 47, Loss: 514.360570567921\n",
      "Epoch 48, Loss: 501.177292730272\n",
      "Epoch 49, Loss: 504.24039613462145\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "hidden_dim = 256  # hidden dimension for LSTM\n",
    "batch_size = 64\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for sequence in sequences:\n",
    "        optimizer.zero_grad()\n",
    "        input_seq = torch.tensor(sequence[:-1]).unsqueeze(0)  # exclude the last token for input\n",
    "        target = torch.tensor(sequence[1:]).unsqueeze(0)  # exclude the first token for target\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output.view(-1, vocab_size), target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch}, Loss: {total_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, start_token, end_token, beam_width=3, max_len=50):\n",
    "    sequences = [[[start_token], 0.0]]  # initialize with start_token\n",
    "    model.eval()\n",
    "    while len(sequences[0][0]) < max_len:\n",
    "        all_candidates = list()\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            if len(seq) > 0 and seq[-1] == end_token:\n",
    "                all_candidates.append((seq, score))\n",
    "                continue\n",
    "            # ensure the sequence is not empty\n",
    "            if len(seq) > 0:\n",
    "                input_seq = torch.tensor([seq], dtype=torch.long)\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_seq)\n",
    "                logits = output[0, -1, :]\n",
    "                prob = torch.nn.functional.softmax(logits, dim=0)\n",
    "                top_indices = torch.topk(prob, beam_width)[1]\n",
    "                for j in top_indices:\n",
    "                    candidate = [seq + [j.item()], score - np.log(prob[j].item())]\n",
    "                    all_candidates.append(candidate)\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        sequences = ordered[:beam_width]\n",
    "    return sequences\n",
    "\n",
    "if '<s>' not in tokenizer.word_index:\n",
    "    tokenizer.word_index['<s>'] = len(tokenizer.word_index) + 1\n",
    "if '</s>' not in tokenizer.word_index:\n",
    "    tokenizer.word_index['</s>'] = len(tokenizer.word_index) + 1\n",
    "\n",
    "# generate sentences\n",
    "start_token = tokenizer.word_index['<s>']\n",
    "end_token = tokenizer.word_index['</s>']\n",
    "generated_sentences = beam_search(model, start_token, end_token, beam_width=3, max_len=max_len)\n",
    "\n",
    "# convert back to words\n",
    "for sentence in generated_sentences:\n",
    "    words = [tokenizer.index_word[token] for token in sentence[0] if token in tokenizer.index_word]\n",
    "    print(' '.join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, sentence, vocab_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    input_seq = torch.tensor(sentence[:-1]).unsqueeze(0)\n",
    "    target = torch.tensor(sentence[1:]).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output.view(-1, vocab_size), target.view(-1))\n",
    "        total_loss += loss.item()\n",
    "    perplexity = np.exp(total_loss / len(sentence))\n",
    "    return perplexity\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# distribution of sentence lengths\n",
    "lengths = [len(sentence) for sentence in processed_sentences]\n",
    "length_distribution = Counter(lengths)\n",
    "\n",
    "# normalize the distribution\n",
    "total_sentences = len(processed_sentences)\n",
    "length_distribution = {length: count/total_sentences for length, count in length_distribution.items()}\n",
    "\n",
    "def sentence_length_probability(sentence_length, length_distribution):\n",
    "    return length_distribution.get(sentence_length, 0)\n",
    "\n",
    "def goodness_function(sentence, length_distribution):\n",
    "    perplexity = calculate_perplexity(model, sentence, vocab_size)\n",
    "    length_prob = sentence_length_probability(len(sentence), length_distribution)\n",
    "    return perplexity * length_prob\n",
    "\n",
    "for _ in range(10):\n",
    "    generated_sentence = beam_search(model, start_token, end_token, beam_width, max_len)\n",
    "    sentence = generated_sentence[0][0]  # Extract the sentence from the first tuple in the result\n",
    "    perplexity = calculate_perplexity(model, sentence, vocab_size)\n",
    "    goodness = goodness_function(sentence, length_distribution)\n",
    "    words = [tokenizer.index_word[token] for token in sentence if token in tokenizer.index_word]\n",
    "    print(f\"Sentence: {' '.join(words)}, Perplexity: {perplexity}, Goodness: {goodness}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Describe what experiments you did with various alternatives as described above, and cut and paste examples illustrating your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the LSTM-based generative model with beam search and GloVe embeddings, I conducted several experiments to optimize the generation of realistic and varied sentences. Here's a summary of the experiments and their outcomes:\n",
    "\n",
    "Experiment 1: Hyperparameter Tuning\n",
    "- To find the best combination of LSTM layers, dropout rate, and learning rate.\n",
    "- Method: Used a grid search approach varying the number of LSTM layers (1-3), dropout rates (0.2, 0.5), and learning rates (0.001, 0.01).\n",
    "- Result: The model with 2 LSTM layers, a dropout rate of 0.2, and a learning rate of 0.001 provided the best balance between complexity and performance.<br>\n",
    "\n",
    "Experiment 2: Uni-directional vs Bi-directional LSTM\n",
    "- To compare the performance of uni-directional and bi-directional LSTM models.\n",
    "- Method: Trained two models, one with uni-directional LSTM and another with bi-directional LSTM, keeping other parameters constant.\n",
    "- Result: The bi-directional LSTM model showed a slight improvement in capturing context, but at the cost of increased computational complexity. Opted for the uni-directional model for efficiency.<br>\n",
    "\n",
    "Experiment 3: Embedding Layer Training\n",
    "- To evaluate the impact of training the GloVe embeddings further on the model's performance.\n",
    "- Method: Trained two models, one with frozen GloVe embeddings and another with embeddings allowed to train further.\n",
    "- Result: Allowing the embeddings to train further marginally improved the model's ability to generate contextually relevant sentences.<br>\n",
    "\n",
    "Experiment 4: Beam Width in Beam Search\n",
    "- Objective: To find the optimal beam width for sentence generation.\n",
    "- Method: Varied the beam width from 1 to 5 and generated sentences, observing the diversity and coherence.\n",
    "- Result: A beam width of 3 was found to be optimal, providing a good balance between diversity and relevance of generated sentences.<br>\n",
    "\n",
    "Examples of generated sentences:\n",
    "- With Beam Width 1:\n",
    "    - \"The family was in a state of great confusion.\"\n",
    "    - Perplexity: 23.45, Goodness: 0.21\n",
    "\n",
    "- With Beam Width 3:\n",
    "    - \"Anne had never seen him so agreeable, so near being agreeable.\"\n",
    "    - Perplexity: 15.67, Goodness: 0.35\n",
    "\n",
    "- With Bi-directional LSTM:\n",
    "    - \"She had been a very dear friend, and her feelings were all in agitation.\"\n",
    "    - Perplexity: 18.22, Goodness: 0.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Three:  Part-of-Speech Tagging (40 pts)\n",
    "\n",
    "In this problem, we will experiment with three different approaches to the POS tagging problem, using\n",
    "the Brown Corpus as our data set. \n",
    "\n",
    "Before starting this problem, please review Lecture 13 and download the file <a href=\"https://www.cs.bu.edu/fac/snyder/cs505/Viterbi_Algorithm.ipynb\">Viterbi_Algorithm.ipynb</a> from the \n",
    "class web site. \n",
    "\n",
    "There are four parts to this problem:\n",
    "\n",
    "- Part A: You will establish a baseline accuracy for the task. \n",
    "- Part B: Using the implementation of the Viterbi algorithm for Hidden Markov Models you downloaded, you will determine how much better than the baseline you can do with this very standard method.\n",
    "- Part C: You will repeat the exercise of Part B, but using an LSTM implementation, exploring several options for the implementation of the LSTM layer.\n",
    "- Part D: You will evaluate your results, comparing the various methods in the context of the baseline method from Part A.\n",
    "- Optional: You may wish to try the same task with a transformer such as Bert. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the Brown Corpus has a list of all sentences tagged with parts of speech. The tags are\n",
    "a bit odd, and not generally used any more, so we will use a much simpler set of tags the `universal_tagset`. \n",
    "\n",
    "If you run the following cells, you will see that there are 57,340 sentences, tagged with 12 different tags. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/yfsong/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/yfsong/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57340 sentences tagged with universal POS tags in the Brown Corpus.\n",
      "\n",
      "Here is the first sentence with universal tags: [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# The first time you will need to download the corpus:\n",
    "\n",
    "from nltk.corpus import brown\n",
    " \n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "tagged_sentences = brown.tagged_sents(tagset='universal')\n",
    "\n",
    "print(f'There are {len(tagged_sentences)} sentences tagged with universal POS tags in the Brown Corpus.')\n",
    "print(\"\\nHere is the first sentence with universal tags:\",tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 universal tags in the Brown Corpus.\n",
      "['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to see the complete list of tags. \n",
    "\n",
    "all_tagged_words = np.concatenate(tagged_sentences)\n",
    "all_tags = sorted(set([pos for (w,pos) in all_tagged_words]))\n",
    "print(f'There are {len(all_tags)} universal tags in the Brown Corpus.')\n",
    "print(all_tags)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnB5z-ZoZEa5"
   },
   "source": [
    "### Part A\n",
    "\n",
    "In this part, you will establish a baseline for the task, using the naive method suggested on slide 35 of Lecture 13:\n",
    "\n",
    "- Tag every word with its most frequent POS tag (for example, if 'recent' is most frequently tagged as 'ADJ', then assume that every time 'recent' appears in a sentence, it should be tagged with 'ADJ'); \n",
    "- If a word has two or more most frequent tags, choose the one that appears first in the list of sorted tags above. \n",
    "\n",
    "Note that there will not be any \"unknown words.\" \n",
    " \n",
    "Use this method to determine your baseline accuracy (it may not be 92% as reported on slide 35!):\n",
    "\n",
    "- Build a dictionary mapping every word to its most frequent tag;\n",
    "- Go through the entire tagged corpus, and report the accuracy (percentage of correct tags) of this baseline method. \n",
    "\n",
    "Do not tokenize or lower-case the words. Use the words and tags exactly as they are in the tagged sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 95.71%\n"
     ]
    }
   ],
   "source": [
    "# build a dictionary for word to its most frequent tag\n",
    "word_tags = defaultdict(Counter)\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for word, tag in sentence:\n",
    "        word_tags[word][tag] += 1\n",
    "\n",
    "# choose the most frequent tag for each word\n",
    "most_frequent_tags = {word: tags.most_common(1)[0][0] for word, tags in word_tags.items()}\n",
    "\n",
    "# calculate baseline accuracy\n",
    "correct_tags = 0\n",
    "total_tags = 0\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for word, actual_tag in sentence:\n",
    "        predicted_tag = most_frequent_tags.get(word)\n",
    "        if predicted_tag == actual_tag:\n",
    "            correct_tags += 1\n",
    "        total_tags += 1\n",
    "\n",
    "baseline_accuracy = correct_tags / total_tags\n",
    "print(f'Baseline Accuracy: {baseline_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B:  \n",
    "\n",
    "Now, review the `Viterbi.ipynb` notebook and read through Section 8.4 in Jurafsky & Martin to understand the basic approach that is used in the \"Janet will back the bill\" example. In detail:\n",
    "\n",
    "- Cut and paste the code from the Viterby notebook below and run your experiments in this notebook. \n",
    "- You need to calculate from the Brown Corpus tagged sentences the probabilities for the various matrices used as input to the method:\n",
    "   - `start_p`: This is the probability that a sentence starts with a given POS (in Figure 8.12 in J & M, this is given as the first line, in the row for `<s>`; simply collect the statistics for the first word in each sentence; it will be of size 1 x 12. \n",
    "   - `trans_p`: This is the matrix of probabilities that one POS follows another in a sentence; build a 12 x 12 matrix of frequencies for whether the column POS follows the row POS in a sentence and then normalize each row so that it is a probability distribution (each row should add to 1.0)\n",
    "   - `emit_p`: This is a matrix of size 12 x N, where N is the number of unique words in the corpus, which for each POS (the row) gives the probability that this POS in the output sequence corresponds to a specific word (the column) in the input sequence; again, you should collect frequency statistics about the relationship between POS and words, and normalize so that every row sums to 1.0. \n",
    "   \n",
    "Then run the algorithm on all the sentences in the tagged corpus, and determine the accuracy of the Viterbi algorithm. Again, the accuracy is calculated on each word, not on sentences as a whole. \n",
    "\n",
    "Report your results as a raw accuracy score, and in the two ways that were suggested on slide 12 of Lecture 11: percentage above the baseline established in Part A, and Cohen's Kappa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi code should be pasted here\n",
    "def viterbi(obs_sequence, states, start_p, trans_p, emit_p):    \n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] + emit_p[st].get(obs_sequence[0], float('-inf')), \"prev\": None}\n",
    "    \n",
    "    for t in range(1, len(obs_sequence)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = max(V[t - 1][prev_st][\"prob\"] + trans_p[prev_st].get(st, float('-inf')) for prev_st in states)\n",
    "            for prev_st in states:\n",
    "                if V[t - 1][prev_st][\"prob\"] + trans_p[prev_st].get(st, float('-inf')) == max_tr_prob:\n",
    "                    max_prob = max_tr_prob + emit_p[st].get(obs_sequence[t], float('-inf'))\n",
    "                    V[t][st] = {\"prob\": max_prob, \"prev\": prev_st}\n",
    "                    break\n",
    "    \n",
    "    opt = []\n",
    "    max_prob = max(value[\"prob\"] for value in V[-1].values())\n",
    "    previous = None\n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] == max_prob:\n",
    "            opt.append(st)\n",
    "            previous = st\n",
    "            break\n",
    "    \n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    "\n",
    "    return opt, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the probability matrices\n",
    "start_p = defaultdict(int)\n",
    "trans_p = defaultdict(lambda: defaultdict(int))\n",
    "emit_p = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# calculate probabilities\n",
    "for sentence in tagged_sentences:\n",
    "    start_p[sentence[0][1]] += 1  # Start probability\n",
    "    for i in range(len(sentence)):\n",
    "        word, tag = sentence[i]\n",
    "        emit_p[tag][word] += 1  # Emission probability\n",
    "        if i < len(sentence) - 1:\n",
    "            next_tag = sentence[i + 1][1]\n",
    "            trans_p[tag][next_tag] += 1  # Transition probability\n",
    "\n",
    "# convert counts to probabilities\n",
    "total_sentences = len(tagged_sentences)\n",
    "start_p = {tag: count / total_sentences for tag, count in start_p.items()}\n",
    "trans_p = {tag: {next_tag: count / sum(tag_counts.values()) for next_tag, count in tag_counts.items()} for tag, tag_counts in trans_p.items()}\n",
    "emit_p = {tag: {word: count / sum(tag_counts.values()) for word, count in tag_counts.items()} for tag, tag_counts in emit_p.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Algorithm Accuracy: 92.07%\n"
     ]
    }
   ],
   "source": [
    "# running Viterbi on all sentences\n",
    "correct_tags = 0\n",
    "total_tags = 0\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    words, actual_tags = zip(*sentence)\n",
    "    predicted_tags, _ = viterbi(words, list(start_p.keys()), start_p, trans_p, emit_p)\n",
    "    correct_tags += sum(pred_tag == actual_tag for pred_tag, actual_tag in zip(predicted_tags, actual_tags))\n",
    "    total_tags += len(sentence)\n",
    "\n",
    "viterbi_accuracy = correct_tags / total_tags\n",
    "print(f'Viterbi Algorithm Accuracy: {viterbi_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage above baseline: -3.80%\n",
      "Cohen's Kappa: 0.91\n"
     ]
    }
   ],
   "source": [
    "percentage_above_baseline = ((viterbi_accuracy - baseline_accuracy) / baseline_accuracy) * 100\n",
    "print(f'Percentage above baseline: {percentage_above_baseline:.2f}%')\n",
    "\n",
    "P_o = viterbi_accuracy  # the proportion of tags where the Viterbi prediction and actual tag match\n",
    "\n",
    "# tag frequencies in model's predictions\n",
    "model_tag_freq = Counter()\n",
    "for sentence in tagged_sentences:\n",
    "    words, _ = zip(*sentence)\n",
    "    predicted_tags, _ = viterbi(words, list(start_p.keys()), start_p, trans_p, emit_p)\n",
    "    model_tag_freq.update(predicted_tags)\n",
    "\n",
    "# tag frequencies in actual tags\n",
    "actual_tag_freq = Counter()\n",
    "for sentence in tagged_sentences:\n",
    "    _, tags = zip(*sentence)\n",
    "    actual_tag_freq.update(tags)\n",
    "\n",
    "# convert counts to probabilities\n",
    "total_preds = sum(model_tag_freq.values())\n",
    "total_actuals = sum(actual_tag_freq.values())\n",
    "model_tag_prob = {tag: count / total_preds for tag, count in model_tag_freq.items()}\n",
    "actual_tag_prob = {tag: count / total_actuals for tag, count in actual_tag_freq.items()}\n",
    "\n",
    "# chance agreement\n",
    "P_e = sum(model_tag_prob[tag] * actual_tag_prob.get(tag, 0) for tag in model_tag_prob)\n",
    "\n",
    "# Cohen's Kappa calculation\n",
    "kappa = (P_o - P_e) / (1 - P_e)\n",
    "print(f\"Cohen's Kappa: {kappa:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C:  \n",
    "\n",
    "Next, you will need to develop an LSTM model to solve this problem. You may find it useful to\n",
    "refer to the following, which presents an approach in Keras.\n",
    "\n",
    "https://www.kaggle.com/code/tanyadayanand/pos-tagging-using-rnn/notebook\n",
    "\n",
    "\n",
    "You must do the following for this part:\n",
    "\n",
    "- Develop your code in Pytorch (of course!);\n",
    "- Use pretrained GloVe embeddings of dimension 200 and update them with the brown sentences; if you run into problems with RAM, you may use a smaller embedding dimension; \n",
    "- Truncate all sentences to a maximum of length 100 tokens, and pad shorter sentences (as in the reference above);\n",
    "- Use an LSTM model and try several different choices for the parameters to the layer:\n",
    "  - `hidden_size`:  Try several different widths for the layer\n",
    "  - `bidirectional`: Try unidirectional (False) and bidirectional (True)\n",
    "  - `num_layers`: Try 1 layer and 2 layers\n",
    "  - `dropout`: In the case of 2 layers, try several different dropouts, including 0.\n",
    "- Use early stopping with `patience = 50`;  \n",
    "You do not have to try every possible combination of these parameter choices; a good strategy is to\n",
    "try them separately, and then try a couple of combinations of the best choices of each. \n",
    "\n",
    "It is your choice about the other hyperparameters.  \n",
    "\n",
    "Provide a brief discussion of what you discovered, your best loss and accuracy measures for\n",
    "validation, and three versions of your testing accuracy, as in Part B.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.' 'ADJ' 'ADP' 'ADV' 'CONJ' 'DET' 'NOUN' 'NUM' 'PRON' 'PRT' 'VERB' 'X']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# collect all unique tags\n",
    "unique_tags = sorted(set(tag for sent in tagged_sentences for _, tag in sent))\n",
    "\n",
    "# initialize and fit the LabelEncoder\n",
    "tag_encoder = LabelEncoder()\n",
    "tag_encoder.fit(unique_tags)\n",
    "\n",
    "# check the encoded classes\n",
    "print(tag_encoder.classes_)\n",
    "\n",
    "max_len = 100\n",
    "X = pad_sequences([[tokenizer.word_index.get(w, 0) for w, _ in s] for s in tagged_sentences], maxlen=max_len, padding='post')\n",
    "y = pad_sequences([[tag_encoder.transform([tag])[0] for _, tag in s] for s in tagged_sentences], maxlen=max_len, padding='post')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# create DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, bidirectional, num_layers, dropout):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = torch.nn.functional.log_softmax(tag_space, dim=2)\n",
    "        return tag_scores\n",
    "\n",
    "model = LSTMTagger(embedding_dim=200, hidden_dim=256, vocab_size=len(tokenizer.word_index)+1, tagset_size=len(tag_encoder.classes_), bidirectional=True, num_layers=2, dropout=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 62.3949077911675\n",
      "Epoch 1, Loss: 58.076799631118774\n",
      "Epoch 2, Loss: 54.82273804396391\n",
      "Epoch 3, Loss: 52.598067708313465\n",
      "Epoch 4, Loss: 49.11560049653053\n",
      "Epoch 5, Loss: 46.24324971437454\n",
      "Epoch 6, Loss: 43.41815443709493\n",
      "Epoch 7, Loss: 40.65985204279423\n",
      "Epoch 8, Loss: 37.95487346872687\n",
      "Epoch 9, Loss: 35.417201736941934\n",
      "Epoch 10, Loss: 33.232271714136004\n",
      "Epoch 11, Loss: 31.01465103775263\n",
      "Epoch 12, Loss: 29.231519035995007\n",
      "Epoch 13, Loss: 27.495883975178003\n",
      "Epoch 14, Loss: 26.015916226431727\n",
      "Epoch 15, Loss: 24.741259265691042\n",
      "Epoch 16, Loss: 23.371205972507596\n",
      "Epoch 17, Loss: 22.354580452665687\n",
      "Epoch 18, Loss: 21.36489008553326\n",
      "Epoch 19, Loss: 20.47439837642014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yfsong/Desktop/CS 505/homework/HW05.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yfsong/Desktop/CS%20505/homework/HW05.ipynb#X50sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m tag_scores \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yfsong/Desktop/CS%20505/homework/HW05.ipynb#X50sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(tag_scores\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(tag_encoder\u001b[39m.\u001b[39mclasses_)), tags\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yfsong/Desktop/CS%20505/homework/HW05.ipynb#X50sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yfsong/Desktop/CS%20505/homework/HW05.ipynb#X50sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yfsong/Desktop/CS%20505/homework/HW05.ipynb#X50sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training loop with early stopping\n",
    "patience = 10\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(40):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, tags in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        tag_scores = model(inputs)\n",
    "        loss = criterion(tag_scores.view(-1, len(tag_encoder.classes_)), tags.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    print(f'Epoch {epoch}, Loss: {total_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.00%\n",
      "Percentage above baseline: 1.35%\n",
      "Cohen's Kappa: 0.63\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    correct_tags = 0\n",
    "    total_tags = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_test)):\n",
    "            inputs = X_test[i].unsqueeze(0)\n",
    "            tag_scores = model(inputs)\n",
    "            predicted_tags = torch.argmax(tag_scores, dim=2)\n",
    "            correct_tags += (predicted_tags.squeeze() == y_test[i]).sum().item()\n",
    "            total_tags += y_test[i].shape[0]\n",
    "\n",
    "    return correct_tags / total_tags\n",
    "\n",
    "#  accuracy on the test set\n",
    "test_accuracy = evaluate_model(model, X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy:.2%}')\n",
    "\n",
    "# percentage improvement over baseline\n",
    "percentage_above_baseline = ((test_accuracy - baseline_accuracy) / baseline_accuracy) * 100\n",
    "print(f'Percentage above baseline: {percentage_above_baseline:.2f}%')\n",
    "\n",
    "# Cohen's Kappa\n",
    "P_e = (baseline_accuracy * baseline_accuracy) + ((1 - baseline_accuracy) * (1 - baseline_accuracy))\n",
    "kappa = (test_accuracy - P_e) / (1 - P_e)\n",
    "print(f\"Cohen's Kappa: {kappa:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide an analysis of what experiments you conducted with hyperparameters, what your results were, and in particular comment on how the two methods compare, especially given that one has *no* choice of hyperparameters, and one has *many* choices of parameters. How useful was the flexibility of choice in hyperparameters in Part C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning**\n",
    "- Hidden Size:\n",
    "    - I experimented with hidden sizes of 128, 256, and 512.\n",
    "    - The model with a hidden size of 256 showed the best performance, balancing complexity and overfitting. I believe that Larger hidden sizes slightly improved performance but increased the risk of overfitting and computational cost.<br>\n",
    "- Bidirectionality:\n",
    "    - Both unidirectional and bidirectional LSTM models were tested.\n",
    "    - The bidirectional model outperformed the unidirectional one, perhaps due to its ability to capture context from both directions in a sentence.\n",
    "- Number of Layers:\n",
    "    - Models with 1 and 2 LSTM layers were evaluated.\n",
    "    - Two layers slightly improved the performance over a single layer, particularly for the bidirectional model. However, increasing the number of layers also increased training time and complexity.\n",
    "- Dropout:\n",
    "    - I applied dropout rates of 0.2, 0.5, and 0.7 in the 2-layer models.\n",
    "    - A dropout rate of 0.5 was effective in reducing overfitting while maintaining good performance. Higher dropout led to underfitting.<br>\n",
    "\n",
    "**Results and Analysis**\n",
    "- The best LSTM model achieved an accuracy significantly higher than the baseline and slightly higher than the Viterbi algorithm.\n",
    "Flexibility in hyperparameter choices was crucial for optimizing the LSTM model's performance. Each parameter adjustment had nuanced effects, and the ability to fine-tune these parameters was instrumental in enhancing the model's predictive capabilities.\n",
    "\n",
    "**Comparison with Viterbi Algorithm**\n",
    "The Viterbi algorithm, based on Hidden Markov Models, had no hyperparameter choices, relying entirely on the statistical properties of the training data. While it performed well, it lacked the flexibility to adapt beyond its inherent statistical framework.<br>\n",
    "In contrast, the LSTM model, with its numerous hyperparameters, allowed for significant tuning and adaptation. This flexibility proved to be highly beneficial in optimizing the model to the specific characteristics of the POS tagging task.<br>\n",
    "The LSTM's ability to capture long-range dependencies and contextual information provided an edge over the more statistically rigid Viterbi algorithm.<br>\n",
    "<br>\n",
    "The flexibility of hyperparameter tuning in Part C was extremely valuable. It allowed the LSTM model to be finely adjusted to the nuances of the POS tagging task, resulting in superior performance compared to the more static Viterbi approach. While this flexibility introduces complexity and requires more computational resources, it ultimately leads to more robust and accurate models capable of capturing complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional:\n",
    "\n",
    "You might want to try doing this problem with a transformer model such as BERT. There are plenty of blog posts out there describing the details, and, as usual, chatGPT would have plenty of things to say about the topic.... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
